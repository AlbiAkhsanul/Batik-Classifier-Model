{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\albia\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from PIL import Image   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv  \n",
    "\n",
    "load_dotenv()\n",
    "datasets_path = os.getenv('DATASET_PATH')\n",
    "models_path = os.getenv('MODELS_PATH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2  \n",
    "batch_size = 16\n",
    "test_split_ratio = 0.2\n",
    "val_split_ratio = 0.2\n",
    "image_size = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = A.Compose([\n",
    "    A.HorizontalFlip(p=0.8),\n",
    "    A.RandomBrightnessContrast(p=0.6),\n",
    "    A.Rotate(limit=15, p=0.7),\n",
    "    A.Affine(scale=(0.95, 1.05), translate_percent=(0.05, 0.05), rotate=15, p=0.6)  \n",
    "])\n",
    "num_augmentations_per_image = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "for class_idx, class_name in enumerate(os.listdir(datasets_path)):\n",
    "    class_folder = os.path.join(datasets_path, class_name)\n",
    "    if not os.path.isdir(class_folder):\n",
    "        continue\n",
    "\n",
    "    for img_name in os.listdir(class_folder):\n",
    "        img_path = os.path.join(class_folder, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, image_size)\n",
    "\n",
    "        # Original image\n",
    "        pil_img = Image.fromarray(image)\n",
    "        tensor = transform(pil_img)\n",
    "        all_images.append(tensor)\n",
    "        all_labels.append(class_idx)\n",
    "\n",
    "        # Augmented images\n",
    "        for _ in range(num_augmentations_per_image):\n",
    "            augmented = augment(image=image)\n",
    "            aug_img = augmented['image']\n",
    "            aug_pil = Image.fromarray(aug_img)\n",
    "            tensor_aug = transform(aug_pil)\n",
    "            all_images.append(tensor_aug)\n",
    "            all_labels.append(class_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.stack(all_images)\n",
    "y = torch.tensor(all_labels)\n",
    "\n",
    "# Step 1: Split awal jadi train_val dan test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=test_split_ratio, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Split train_val jadi train dan validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=val_split_ratio, stratify=y_train_val, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Buat DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples after augmentation: 960\n",
      "Train size: 614, Test size: 192, Val size: 154\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total samples after augmentation: {len(all_images)}\")\n",
    "print(f\"Train size: {len(train_loader.dataset)}, Test size: {len(test_loader.dataset)}, Val size: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah kelas unik di y: 2\n",
      "Distribusi label: Counter({0: 480, 1: 480})\n",
      "Bentuk tensor X (jumlah gambar, channel, tinggi, lebar): torch.Size([960, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(f\"Jumlah kelas unik di y: {len(torch.unique(y))}\")\n",
    "print(\"Distribusi label:\", Counter(y.tolist()))\n",
    "print(f\"Bentuk tensor X (jumlah gambar, channel, tinggi, lebar): {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "class EfficientNetCustom(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EfficientNetCustom, self).__init__()\n",
    "\n",
    "        # Definisikan weights sebelum digunakan\n",
    "        weights = EfficientNet_B0_Weights.DEFAULT\n",
    "        self.model = efficientnet_b0(weights=weights)\n",
    "\n",
    "        # Ambil fitur backbone dan avgpool dari model\n",
    "        self.features = self.model.features\n",
    "        self.avgpool = self.model.avgpool\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Ambil jumlah fitur input classifier dari model asli\n",
    "        in_features = self.model.classifier[1].in_features\n",
    "\n",
    "        # Buat classifier custom\n",
    "        self.fc1 = nn.Linear(in_features, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\albia\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60], Loss: 0.6732, Val Acc: 0.8125\n",
      "Epoch [2/60], Loss: 0.5927, Val Acc: 0.8958\n",
      "Epoch [3/60], Loss: 0.4838, Val Acc: 0.9219\n",
      "Epoch [4/60], Loss: 0.3657, Val Acc: 0.9167\n",
      "Epoch [5/60], Loss: 0.2728, Val Acc: 0.9323\n",
      "Epoch [6/60], Loss: 0.1938, Val Acc: 0.9427\n",
      "Epoch [7/60], Loss: 0.1272, Val Acc: 0.9375\n",
      "Epoch [8/60], Loss: 0.1234, Val Acc: 0.9479\n",
      "Epoch [9/60], Loss: 0.0925, Val Acc: 0.9635\n",
      "Epoch [10/60], Loss: 0.0801, Val Acc: 0.9635\n",
      "Epoch [11/60], Loss: 0.0813, Val Acc: 0.9688\n",
      "Epoch [12/60], Loss: 0.0569, Val Acc: 0.9688\n",
      "Epoch [13/60], Loss: 0.0610, Val Acc: 0.9635\n",
      "Epoch [14/60], Loss: 0.0443, Val Acc: 0.9740\n",
      "Epoch [15/60], Loss: 0.0410, Val Acc: 0.9792\n",
      "Epoch [16/60], Loss: 0.0311, Val Acc: 0.9740\n",
      "Epoch [17/60], Loss: 0.0251, Val Acc: 0.9740\n",
      "Epoch [18/60], Loss: 0.0480, Val Acc: 0.9792\n",
      "Epoch [19/60], Loss: 0.0236, Val Acc: 0.9688\n",
      "Epoch [20/60], Loss: 0.0380, Val Acc: 0.9792\n",
      "Epoch [21/60], Loss: 0.0278, Val Acc: 0.9792\n",
      "Epoch [22/60], Loss: 0.0290, Val Acc: 0.9740\n",
      "Epoch [23/60], Loss: 0.0398, Val Acc: 0.9740\n",
      "Epoch [24/60], Loss: 0.0231, Val Acc: 0.9792\n",
      "Epoch [25/60], Loss: 0.0357, Val Acc: 0.9792\n",
      "Epoch [26/60], Loss: 0.0242, Val Acc: 0.9792\n",
      "Epoch [27/60], Loss: 0.0298, Val Acc: 0.9792\n",
      "Epoch [28/60], Loss: 0.0330, Val Acc: 0.9792\n",
      "Epoch [29/60], Loss: 0.0215, Val Acc: 0.9792\n",
      "Epoch [30/60], Loss: 0.0192, Val Acc: 0.9792\n",
      "Epoch [31/60], Loss: 0.0253, Val Acc: 0.9792\n",
      "Epoch [32/60], Loss: 0.0165, Val Acc: 0.9792\n",
      "Epoch [33/60], Loss: 0.0292, Val Acc: 0.9792\n",
      "Epoch [34/60], Loss: 0.0337, Val Acc: 0.9792\n",
      "Epoch [35/60], Loss: 0.0691, Val Acc: 0.9792\n",
      "Epoch [36/60], Loss: 0.0334, Val Acc: 0.9792\n",
      "Epoch [37/60], Loss: 0.0424, Val Acc: 0.9792\n",
      "⛔ Early stopping triggered at epoch 37\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "model = EfficientNetCustom(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0015\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                       factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Inisialisasi early stopping\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop_patience = 5\n",
    "num_epochs = 60\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    val_acc = evaluate(model, test_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    if epoch_loss < best_loss - 1e-4:\n",
    "        best_loss = epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model, 'best_model_cnn.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"⛔ Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9792    0.9792    0.9792        96\n",
      "           1     0.9792    0.9792    0.9792        96\n",
      "\n",
      "    accuracy                         0.9792       192\n",
      "   macro avg     0.9792    0.9792    0.9792       192\n",
      "weighted avg     0.9792    0.9792    0.9792       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Evaluasi klasifikasi\n",
    "print(classification_report(all_labels, all_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), models_path+'/CNN2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
