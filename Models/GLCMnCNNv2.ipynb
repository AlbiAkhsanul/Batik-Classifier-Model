{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv  \n",
    "\n",
    "load_dotenv()\n",
    "datasets_path = os.getenv('AUGMENTED_PATH_BALI_PEKALONGAN')\n",
    "models_path = os.getenv('MODELS_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bali', 'pekalongan']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(datasets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "batch_size = 16\n",
    "test_split_ratio = 0.2\n",
    "val_split_ratio = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset With GLCM Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "class GLCMCNNHybridDataset(Dataset):\n",
    "    def __init__(self, image_folder_dataset, transform=None):\n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.transform = transform\n",
    "        # Simpan fitur GLCM dan label\n",
    "        self.glcm_features = []\n",
    "        self.labels = []\n",
    "\n",
    "        print(\"ðŸ”„ Preprocessing GLCM features...\")\n",
    "\n",
    "        for idx in range(len(image_folder_dataset)):\n",
    "            image, label = image_folder_dataset[idx]\n",
    "\n",
    "            # Simpan label\n",
    "            self.labels.append(label)\n",
    "\n",
    "            # Transform dulu (pakai transform yg sama dengan training)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            # Convert tensor (C, H, W) -> numpy image (H, W, C)\n",
    "            image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "            image_np = (image_np * 255).astype(np.uint8)\n",
    "\n",
    "            # Ekstrak GLCM\n",
    "            gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "            distances = [1, 2, 3]\n",
    "            angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "            glcm = graycomatrix(gray, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "            props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
    "            features = []\n",
    "            for prop in props:\n",
    "                values = graycoprops(glcm, prop)\n",
    "                features.extend(values.flatten())\n",
    "            features = np.array(features, dtype=np.float32)\n",
    "\n",
    "            # Normalisasi fitur (z-score)\n",
    "            features = (features - features.mean()) / (features.std() + 1e-8)\n",
    "            self.glcm_features.append(features)\n",
    "\n",
    "        # Konversi list ke tensor\n",
    "        self.glcm_features = torch.tensor(np.array(self.glcm_features), dtype=torch.float32)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "        print(\"âœ… GLCM features extracted and cached.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, _ = self.image_folder_dataset[index]  # label diambil dari self.labels\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        glcm_feature = self.glcm_features[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return image, glcm_feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Preprocessing GLCM features...\n",
      "âœ… GLCM features extracted and cached.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Buat dataset dasar\n",
    "base_dataset = datasets.ImageFolder(root=datasets_path)\n",
    "hybrid_dataset = GLCMCNNHybridDataset(base_dataset, transform=transform)\n",
    "\n",
    "# Ambil semua label\n",
    "targets = base_dataset.targets\n",
    "indices = list(range(len(base_dataset)))\n",
    "\n",
    "# Pertama: Bagi data menjadi train+val dan test\n",
    "trainval_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=test_split_ratio,\n",
    "    stratify=targets,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Ambil label untuk data trainval\n",
    "trainval_targets = [targets[i] for i in trainval_indices]\n",
    "\n",
    "# Kedua: Bagi trainval menjadi train dan val\n",
    "train_indices, val_indices = train_test_split(\n",
    "    trainval_indices,\n",
    "    test_size=val_split_ratio,\n",
    "    stratify=trainval_targets,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Buat subset dataset-nya\n",
    "train_dataset = Subset(hybrid_dataset, train_indices)\n",
    "val_dataset = Subset(hybrid_dataset, val_indices)\n",
    "test_dataset = Subset(hybrid_dataset, test_indices)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape GLCM Features: torch.Size([16, 60])\n"
     ]
    }
   ],
   "source": [
    "# Cek dimensi glcm_features\n",
    "for images, glcm_features, labels in train_loader:\n",
    "    print(\"Shape GLCM Features:\", glcm_features.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi kelas (train): Counter({0: 307, 1: 307})\n",
      "Distribusi kelas (test): Counter({1: 96, 0: 96})\n",
      "Distribusi kelas (val): Counter({0: 77, 1: 77})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Distribusi kelas (train):\", Counter([base_dataset.targets[i] for i in train_indices]))\n",
    "print(\"Distribusi kelas (test):\", Counter([base_dataset.targets[i] for i in test_indices]))\n",
    "print(\"Distribusi kelas (val):\", Counter([base_dataset.targets[i] for i in val_indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model (EfficientNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EfficientNet-B0\n",
    "weights = EfficientNet_B0_Weights.DEFAULT\n",
    "cnn_model = efficientnet_b0(weights=weights)\n",
    "cnn_feature_size = cnn_model.classifier[1].in_features  # EfficientNet features\n",
    "\n",
    "# Replace classification layer to get features\n",
    "cnn_model.classifier = nn.Identity()\n",
    "cnn_model = cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n"
     ]
    }
   ],
   "source": [
    "print(cnn_feature_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PararelModel(nn.Module):\n",
    "    def __init__(self, cnn_model, glcm_feature_size, cnn_feature_size, num_classes):\n",
    "        super(PararelModel, self).__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.fc1 = nn.Linear(glcm_feature_size + cnn_feature_size, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, image, glcm_features):\n",
    "        cnn_features = self.cnn_model(image)\n",
    "        combined_features = torch.cat((glcm_features, cnn_features), dim=1)\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleParallelModel(nn.Module):\n",
    "    def __init__(self, cnn_model, glcm_feature_size, cnn_feature_size, num_classes):\n",
    "        super(SimpleParallelModel, self).__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.fc1 = nn.Linear(glcm_feature_size + cnn_feature_size, 256)  # Menurunkan dimensi\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Langsung ke output class\n",
    "\n",
    "    def forward(self, image, glcm_features):\n",
    "        cnn_features = self.cnn_model(image)\n",
    "        combined_features = torch.cat((glcm_features, cnn_features), dim=1)\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "glcm_feature_size = 60  # Number of GLCM features\n",
    "# model = PararelModel(cnn_model, glcm_feature_size, cnn_feature_size, num_classes).to(device)\n",
    "model = SimpleParallelModel(cnn_model, glcm_feature_size, cnn_feature_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\albia\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.6721, Val Acc: 0.7917\n",
      "Epoch [2/50], Loss: 0.5696, Val Acc: 0.8125\n",
      "Epoch [3/50], Loss: 0.4048, Val Acc: 0.8177\n",
      "Epoch [4/50], Loss: 0.2860, Val Acc: 0.8802\n",
      "Epoch [5/50], Loss: 0.2059, Val Acc: 0.8854\n",
      "Epoch [6/50], Loss: 0.1536, Val Acc: 0.9375\n",
      "Epoch [7/50], Loss: 0.1221, Val Acc: 0.9531\n",
      "Epoch [8/50], Loss: 0.0691, Val Acc: 0.9427\n",
      "Epoch [9/50], Loss: 0.0731, Val Acc: 0.9583\n",
      "Epoch [10/50], Loss: 0.0840, Val Acc: 0.9688\n",
      "Epoch [11/50], Loss: 0.0902, Val Acc: 0.9635\n",
      "Epoch [12/50], Loss: 0.0438, Val Acc: 0.9688\n",
      "Epoch [13/50], Loss: 0.0467, Val Acc: 0.9792\n",
      "Epoch [14/50], Loss: 0.0400, Val Acc: 0.9792\n",
      "Epoch [15/50], Loss: 0.0317, Val Acc: 0.9844\n",
      "Epoch [16/50], Loss: 0.0644, Val Acc: 0.9844\n",
      "Epoch [17/50], Loss: 0.0515, Val Acc: 0.9844\n",
      "Epoch [18/50], Loss: 0.0604, Val Acc: 0.9844\n",
      "Epoch [19/50], Loss: 0.0270, Val Acc: 0.9844\n",
      "Epoch [20/50], Loss: 0.0359, Val Acc: 0.9844\n",
      "Epoch [21/50], Loss: 0.0372, Val Acc: 0.9896\n",
      "Epoch [22/50], Loss: 0.0323, Val Acc: 0.9792\n",
      "Epoch [23/50], Loss: 0.0266, Val Acc: 0.9844\n",
      "Epoch [24/50], Loss: 0.0197, Val Acc: 0.9844\n",
      "Epoch [25/50], Loss: 0.0145, Val Acc: 0.9792\n",
      "Epoch [26/50], Loss: 0.0271, Val Acc: 0.9896\n",
      "Epoch [27/50], Loss: 0.0226, Val Acc: 0.9844\n",
      "Epoch [28/50], Loss: 0.0292, Val Acc: 0.9844\n",
      "Epoch [29/50], Loss: 0.0232, Val Acc: 0.9844\n",
      "Epoch [30/50], Loss: 0.0214, Val Acc: 0.9844\n",
      "â›” Early stopping triggered at epoch 30\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "patience = 5  # jumlah epoch tanpa perbaikan sebelum berhenti\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0015\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                       factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, glcm_features, labels in data_loader:\n",
    "            images, glcm_features, labels = images.to(device), glcm_features.to(device), labels.to(device)\n",
    "            outputs = model(images, glcm_features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Training loop dengan early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, glcm_features, labels in train_loader:\n",
    "        images, glcm_features, labels = images.to(device), glcm_features.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, glcm_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    val_acc = evaluate(model, test_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if epoch_loss < best_loss - 1e-4:\n",
    "        best_loss = epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model_hybrid.pth')  # Simpan model terbaik\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"â›” Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0:\n",
      "  Precision: 0.9794\n",
      "  Recall:    0.9896\n",
      "  F1-Score:  0.9845\n",
      "------------------------------\n",
      "Class 1:\n",
      "  Precision: 0.9895\n",
      "  Recall:    0.9792\n",
      "  F1-Score:  0.9843\n",
      "------------------------------\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98        96\n",
      "           1       0.99      0.98      0.98        96\n",
      "\n",
      "    accuracy                           0.98       192\n",
      "   macro avg       0.98      0.98      0.98       192\n",
      "weighted avg       0.98      0.98      0.98       192\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAJOCAYAAAAqFJGJAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJk1JREFUeJzt3QmYnuO9+PFf9iDEEgS11M6htpSilaqUtqcIB1VVIUUpaknaUrUklmhRpJao2rdDq1TRpiVVS63R5FhiTdROIiRNQtb5X/dzzuSfSYKJX2beSebzua65knne7X5HT877nfu+n6dNXV1dXQAAACS0zTwYAACgEBYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAK3ICy+8EDvvvHN07do12rRpE7fddttCff6XX365et6rrrpqoT7vouzLX/5y9QWwuBMWAM3spZdeiu9///ux9tprR+fOnWOZZZaJ7bffPi644IL44IMPmvS1+/TpE08++WScccYZce2110aPHj1icXHggQdWUVN+nvP7OZaoKreXr3POOWeBn/+NN96IU089NUaMGLGQRgyweGlf6wEAtCZ33nln7L333tGpU6c44IADYpNNNolp06bFAw88ED/60Y/i6aefjl//+tdN8trlw/ZDDz0UJ554Yhx55JFN8hprrrlm9TodOnSIWmjfvn1MmTIl/vjHP8Y+++zT4Lbrr7++CrkPP/zwUz13CYsBAwbEWmutFZtvvnmjH/eXv/zlU70ewKJGWAA0kzFjxsS+++5bffgeNmxYrLLKKrNvO+KII+LFF1+swqOpjB07tvpz2WWXbbLXKLMB5cN7rZRgK7M/N9544zxhccMNN8R//ud/xi233NIsYymBs+SSS0bHjh2b5fUAas1SKIBm8otf/CImTZoUl19+eYOoqLfuuuvG0UcfPfv7GTNmxGmnnRbrrLNO9YG5/Kb8pz/9aUydOrXB48rxb37zm9Wsx9Zbb119sC/LrK655prZ9ylLeErQFGVmpARAeVz9EqL6v8+pPKbcb05//etf44tf/GIVJ126dIkNNtigGtMn7bEoIfWlL30pllpqqeqxu+++e4waNWq+r1cCq4yp3K/sBTnooIOqD+mNtd9++8Wf/vSneP/992cfe+yxx6qlUOW2uY0fPz769+8fm266afWeylKqr3/96zFy5MjZ97n33nvj85//fPX3Mp76JVX177PsoSizT8OHD48ddtihCor6n8vceyzKcrTy32ju97/LLrvEcsstV82MACyKhAVAMynLc8oH/u22265R9z/44IPj5JNPji233DLOO++86NmzZwwaNKia9Zhb+TC+1157xVe/+tU499xzqw+o5cN5WVpV7LnnntVzFN/+9rer/RXnn3/+Ao2/PFcJmBI2AwcOrF5nt912iwcffPBjH3f33XdXH5rfeeedKh6OO+64+Mc//lHNLJQQmVuZafj3v/9dvdfy9/LhvSxBaqzyXsuH/t///vcNZis23HDD6mc5t9GjR1eb2Mt7++Uvf1mFV9mHUn7e9R/yN9poo+o9F4ceemj18ytfJSLqvfvuu1WQlGVS5We74447znd8ZS/NiiuuWAXGzJkzq2OXXnpptWTqV7/6Vay66qqNfq8ALUodAE1uwoQJdeWf3N13371R9x8xYkR1/4MPPrjB8f79+1fHhw0bNvvYmmuuWR277777Zh9755136jp16lTXr1+/2cfGjBlT3e/ss89u8Jx9+vSpnmNup5xySnX/euedd171/dixYz9y3PWvceWVV84+tvnmm9ettNJKde++++7sYyNHjqxr27Zt3QEHHDDP6/Xt27fBc+6xxx51K6ywwke+5pzvY6mllqr+vtdee9XttNNO1d9nzpxZ171797oBAwbM92fw4YcfVveZ+32Un9/AgQNnH3vsscfmeW/1evbsWd02ZMiQ+d5WvuY0dOjQ6v6nn3563ejRo+u6dOlS17t37098jwAtmRkLgGYwceLE6s+ll166Ufe/6667qj/Lb/fn1K9fv+rPufdibLzxxtVSo3rlN+JlmVL5bfzCUr834w9/+EPMmjWrUY958803q7MoldmT5Zdffvbxz33uc9XsSv37nNNhhx3W4PvyvspsQP3PsDHKkqeyfOmtt96qlmGVP+e3DKooy8zatv3f/3dYZhDKa9Uv83riiSca/ZrlecoyqcYop/wtZwYrsyBlhqUsjSqzFgCLMmEB0AzKuv2iLPFpjH/961/Vh92y72JO3bt3rz7gl9vntMYaa8zzHGU51HvvvRcLy7e+9a1q+VJZorXyyitXS7Juvvnmj42M+nGWD+lzK8uLxo0bF5MnT/7Y91LeR7Eg7+Ub3/hGFXE33XRTdTaosj9i7p9lvTL+skxsvfXWq+KgW7duVZj9z//8T0yYMKHRr7naaqst0EbtcsrbElslvAYPHhwrrbRSox8L0BIJC4BmCouydv6pp55aoMfNvXn6o7Rr126+x+vq6j71a9Sv/6+3xBJLxH333Vftmfjud79bffAusVFmHua+b0bmvdQrgVBmAq6++uq49dZbP3K2ojjzzDOrmaGyX+K6666LoUOHVpvU/+M//qPRMzP1P58F8c9//rPad1KUPR0AizphAdBMyubgcnG8ci2JT1LO4FQ+1JYzGc3p7bffrs52VH+Gp4WhzAjMeQalenPPihRlFmWnnXaqNjk/88wz1YX2ylKjv/3tbx/5PornnntuntueffbZanagnCmqKZSYKB/eyyzR/Da81/vd735XbbQuZ+sq9yvLlHr16jXPz6SxkdcYZZamLJsqS9jKZvByxrBy5iqARZmwAGgmP/7xj6sP0WUpUQmEuZXoKGcMql/KU8x95qbygb4o12NYWMrpbMuSnzIDMefeiPKb/rlPyzq3+gvFzX0K3HrltLrlPmXmYM4P6mXmppwFqf59NoUSC+V0vRdeeGG1hOzjZkjmng357W9/G6+//nqDY/UBNL8IW1A/+clP4pVXXql+LuW/aTndbzlL1Ef9HAEWBS6QB9BMygf4ctrTsnyo7C+Y88rb5fSr5cNs2eRcbLbZZtUHzXIV7vJBtpz69NFHH60+iPbu3fsjT2X6aZTf0pcPunvssUf88Ic/rK4Zcckll8T666/fYPNy2WhclkKVqCkzEWUZz8UXXxyf+cxnqmtbfJSzzz67Og3rtttuG9/73veqK3OX06qWa1SU0882lTK78rOf/axRM0nlvZUZhHIq4LIsqezLKKcGnvu/X9nfMmTIkGr/RgmNbbbZJj772c8u0LjKDE/5uZ1yyimzT3975ZVXVte6OOmkk6rZC4BFkRkLgGZUrvtQZgbKNSfK2ZXKFbePP/746noO5boQZRNvvd/85jfV9RvKEpljjjmm+kB6wgknxH//938v1DGtsMIK1exEuahbmVUp8VKuIbHrrrvOM/aysfqKK66oxn3RRRdV+xLKuEokfJSyrOjPf/5z9Trluhxl0/IXvvCF6voXC/qhvCmUC9mVs22VvRXlAoUlpspZt1ZfffUG9+vQoUP1sykzHOXMVeV6IH//+98X6LXKsqy+ffvGFltsESeeeGKDM1+V1y7/G3j44YcX2nsDaE5tyjlnm/UVAQCAxY4ZCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACAtMXyyttLbHFkrYcAQCO899iFtR4CAJ+gcyOLwYwFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQFr7/FMAC0uXJTvFKT/4Zuz2lc1ixeW6xMjnXov+v/hdDH/mler2Xw/YP7672xcaPOYvDz4Tux95cY1GDMDwxx+Lq664PEY981SMHTs2zht8UXxlp161HhY0O2EBLcglJ+8XG6+7avT92dXx5tgJ8e1vbB13Djkqtvyv0+ONsROq+wx98On4/inXzX7M1GkzajhiAD74YEpssMEG0XvP/4rjjj6y1sOBmhEW0EJ07tQheu+0eex97K/jwSdeqo6dceld8Y0dNolD9v5SDLj4jurYtGkz4u13/13j0QJQ74tf6ll9QWsnLKCFaN+ubbRv3y4+nDa9wfEPp06P7bZYZ/b3X+qxXvzrnkHx/sQpce9jz8eAi+6I8RMm12DEAAAtJCzGjRsXV1xxRTz00EPx1ltvVce6d+8e2223XRx44IGx4oor1nJ40KwmTZkaD48cHScc8vV4bszb8fa7E2Ofr/WIbT732Xjp1bHVff76j1Hxh2Ej4+XX3421P9MtBhy1a/zhwsOjZ59zY9asulq/BQCgFatZWDz22GOxyy67xJJLLhm9evWK9ddfvzr+9ttvx+DBg+Oss86KoUOHRo8ePT72eaZOnVp9zalu1sxo07Zdk44fmkLfn10Tl576nRj9lzNixoyZMeLZV+PmPz8eW2y0RnX7b4cOn33fp198I5584fUYdceA2KHHenHvo8/XcOQAQGtXs7A46qijYu+9944hQ4ZEmzZtGtxWV1cXhx12WHWfMpvxcQYNGhQDBgxocKzdyp+PDqts3STjhqY05rVxsfPBF8SSnTvGMl06x1vjJsa1Zx0UY14fN9/7l5mLse/9O9ZZfUVhAQC0zutYjBw5Mo499th5oqIox8ptI0aM+MTnOeGEE2LChAkNvtqvvFUTjRqax5QPp1VRsezSS0Sv7TaKO+59cr73W22lZWOFrktV9wUAaJUzFmUvxaOPPhobbrjhfG8vt6288sqf+DydOnWqvuZkGRSLql7bbhSltZ9/+Z1qFuLMY3vH82PejmtufyiWWqJjnPj9b8Rt94yoQmLt1bvFGUf3jpdeHVftvQCgNqZMnhyvvPK/1xsqXn/ttXh21Kjo2rVrrLLqqjUdG7SKsOjfv38ceuihMXz48Nhpp51mR0TZY3HPPffEZZddFuecc06thgc10bVL5xh41G6x2srLxvgJU+IP94yIUy76Y8yYMSvat6uLTdZbLb6z6zbVTEa5zsXdDz0bAy++I6ZNdy0LgFp5+umn4uCDDpj9/Tm/GFT9udvue8RpZ55Vw5FB82pTVzY01MhNN90U5513XhUXM2fOrI61a9cuttpqqzjuuONin332+VTPu8QWLk4DsCh477ELaz0EAD5B5/aLQFjUmz59enXq2aJbt27RoUOH1PMJC4BFg7AAWHzCokVcIK+ExCqrrFLrYQAAAIvaWaEAAIDFh7AAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAAahMW999/f+y///6x7bbbxuuvv14du/baa+OBBx7IjwgAAFj8w+KWW26JXXbZJZZYYon45z//GVOnTq2OT5gwIc4888ymGCMAALC4hcXpp58eQ4YMicsuuyw6dOgw+/j2228fTzzxxMIeHwAAsDiGxXPPPRc77LDDPMe7du0a77///sIaFwAAsDiHRffu3ePFF1+c53jZX7H22msvrHEBAACLc1gccsghcfTRR8cjjzwSbdq0iTfeeCOuv/766N+/fxx++OFNM0oAAKBFa7+gDzj++ONj1qxZsdNOO8WUKVOqZVGdOnWqwuKoo45qmlECAAAtWpu6urq6T/PAadOmVUuiJk2aFBtvvHF06dIlWooltjiy1kMAoBHee+zCWg8BgE/QuX0TzVjU69ixYxUUAAAACxwWO+64Y7W34qMMGzYsOyYAAGBxD4vNN9+8wffTp0+PESNGxFNPPRV9+vRZmGMDAAAW17A477zz5nv81FNPrfZbAAAArc8Cn272o+y///5xxRVXLKynAwAAWmNYPPTQQ9G5c+eF9XQAAMDivBRqzz33bPB9OVvtm2++GY8//nicdNJJ0RK8++ivaj0EABphuW2OrvUQAPgEHwy/IJokLLp27drg+7Zt28YGG2wQAwcOjJ133nlBnw4AAFgMLFBYzJw5Mw466KDYdNNNY7nllmu6UQEAAIvvHot27dpVsxLvv/9+040IAABY/Ddvb7LJJjF69OimGQ0AANA6wuL000+P/v37xx133FFt2p44cWKDLwAAoPVpU1dO69QIZXN2v379Yumll/7/D27TZvbfy9OU78s+jFqbMr1RbwmAGlvhC8fUeggALKSzQjU6LMr+ijJDMWrUqI+9X8+ePaPWhAXAokFYALTC083W90dLCAcAAGAR3mMx59InAACAT3Udi/XXX/8T42L8+PEL8pQAAEBrC4sBAwbMc+VtAACABQqLfffdN1ZaaaWmGw0AALB477GwvwIAAEiHRSPPSgsAALRCjV4KNWvWrKYdCQAA0DpONwsAADA/wgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YQAt2+WWXxne+tVdsv/WW8ZUdtotjf3hEvDxmdK2HBdCqdVmyU5zdb4947o5TYvyDZ8ffrjgmttp4jfned/AJ+8QHwy+II7/ds9nHCc1NWEAL9sTjj8W3vr1fXHPDTXHJr6+IGdNnxOGHHhwfTJlS66EBtFqXnLRvfGWbDaLvSddFj2/9PO5++Nm485IfxKordm1wv912/Fxsvema8cY779dsrNCchAW0YBdd+pvYrfeesc6668UGG24YA84YFG+9+UY888zTtR4aQKvUuVOH6P2VzeLEwbfHg/98KUa/Ni7O+PWf46VXx8Uhe20/+34lMn75o/+Kg352bUyfMbOmY4bmIixgETJp0r+rP7t2bfhbMQCaR/t2baN9+3bx4dQZDY5/OHV6bLf52tXf27RpE5eftn+cd+2wGDX6rRqNFJpfiw6LV199Nfr27VvrYUCLMGvWrDjnrDNj8y22jHXXW7/WwwFolSZNmRoPjxwTJxy8c6zSbZlo27ZN7Pv1HrHNpmtF927LVPfpd+BOMWPmrLjoxr/XerjQrFp0WIwfPz6uvvrqj73P1KlTY+LEiQ2+yjFY3Aw6fWC8+OILcdbZv6z1UABatb4nX1vNSoweelpMeOjcOGLfHeLmoU/ErLq62GLDz8QR+/aMQ0+5vtbDhGbXpq6uri5q5Pbbb//Y20ePHh39+vWLmTM/em3iqaeeGgMGDGhw7Kc/OzlOPPnUhTZOqLWzzhgY9w4bFpdffV2s9pnP1Ho4sNCs8IVjaj0E+NSW7NwxlunSOd4aNzGuHdQnllqyUwx7+Ln4+XG9Y9as///xqiydmjlzVrz29nux4a4Dazpm+DTKmc1afFi0bdu2Kv6PG0K5/ePCosxOzD1DMbNtx+jUqdNCHSvUQvm/jZ+feVoMu+fuuOzKa2LNNdeq9ZBgoRIWLA6WXXqJGPXHk+PEC26P24aNjO7dGu6D++OFh8UNdz0e19z+SLzwr3dqNk5o6rBoHzW0yiqrxMUXXxy77777fG8fMWJEbLXVVh/7HCUg5o6IKdNr1kqw0Jc//emuO+K8wRfFUkstFePGja2Od+mydHTu3LnWwwNolXptu2G0iYjn//VOrLP6inHm0bvF8y+/E9f88ZGYMWNWjJ/Q8JTg5axQb4+bKCpY7NU0LEo0DB8+/CPD4pNmM2Bx99ubbqz+POSgAxocH3D6mdVpaAFofl27dI6BR+4aq620bIyfODn+cM/IOOXiO6uogNaspkuh7r///pg8eXJ87Wtfm+/t5bbHH388evZcsKtVmrEAWDRYCgXQ8i0SeyyairAAWDQIC4DFJyxa9OlmAQCARYOwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEgTFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACANGEBAACkCQsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAAAgTVgAAABpwgIAAEhrU1dXV5d/GqApTZ06NQYNGhQnnHBCdOrUqdbDAWA+/FtNaycsYBEwceLE6Nq1a0yYMCGWWWaZWg8HgPnwbzWtnaVQAABAmrAAAADShAUAAJAmLGARUDYBnnLKKTYDArRg/q2mtbN5GwAASDNjAQAApAkLAAAgTVgAAABpwgJauIsuuijWWmut6Ny5c2yzzTbx6KOP1npIAMzhvvvui1133TVWXXXVaNOmTdx22221HhLUhLCAFuymm26K4447rjrLyBNPPBGbbbZZ7LLLLvHOO+/UemgA/J/JkydX/z6XXwRBa+asUNCClRmKz3/+83HhhRdW38+aNStWX331OOqoo+L444+v9fAAmEuZsbj11lujd+/etR4KNDszFtBCTZs2LYYPHx69evWafaxt27bV9w899FBNxwYAMDdhAS3UuHHjYubMmbHyyis3OF6+f+utt2o2LgCA+REWAABAmrCAFqpbt27Rrl27ePvttxscL9937969ZuMCAJgfYQEtVMeOHWOrrbaKe+65Z/axsnm7fL/tttvWdGwAAHNrP88RoMUop5rt06dP9OjRI7beeus4//zzq9MaHnTQQbUeGgD/Z9KkSfHiiy/O/n7MmDExYsSIWH755WONNdao6digOTndLLRw5VSzZ599drVhe/PNN4/BgwdXp6EFoGW49957Y8cdd5znePnF0FVXXVWTMUEtCAsAACDNHgsAACBNWAAAAGnCAgAASBMWAABAmrAAAADShAUAAJAmLAAAgDRhAQAApAkLAJrcgQceGL179579/Ze//OU45phjanKF5DZt2sT777/f7K8NsLgTFgCt/AN/+aBdvjp27BjrrrtuDBw4MGbMmNGkr/v73/8+TjvttEbdVwwALBra13oAANTW1772tbjyyitj6tSpcdddd8URRxwRHTp0iBNOOKHB/aZNm1bFx8Kw/PLLL5TnAaDlMGMB0Mp16tQpunfvHmuuuWYcfvjh0atXr7j99ttnL18644wzYtVVV40NNtiguv+rr74a++yzTyy77LJVIOy+++7x8ssvz36+mTNnxnHHHVfdvsIKK8SPf/zjqKura/Cacy+FKlHzk5/8JFZfffVqPGXm5PLLL6+ed8cdd6zus9xyy1UzF2VcxaxZs2LQoEHx2c9+NpZYYonYbLPN4ne/+12D1ymhtP7661e3l+eZc5wALFzCAoAGyofwMjtR3HPPPfHcc8/FX//617jjjjti+vTpscsuu8TSSy8d999/fzz44IPRpUuXataj/jHnnntuXHXVVXHFFVfEAw88EOPHj49bb731Y1/zgAMOiBtvvDEGDx4co0aNiksvvbR63hIat9xyS3WfMo4333wzLrjggur7EhXXXHNNDBkyJJ5++uk49thjY//994+///3vswNozz33jF133TVGjBgRBx98cBx//PFN/NMDaL0shQKgUmYVSkgMHTo0jjrqqBg7dmwstdRS8Zvf/Gb2Eqjrrruumikox8rsQVGWUZXZibIXYuedd47zzz+/WkZVPtQX5YN/ec6P8vzzz8fNN99cxUuZLSnWXnvteZZNrbTSStXr1M9wnHnmmXH33XfHtttuO/sxJWRKlPTs2TMuueSSWGeddarQKcqMy5NPPhk///nPm+gnCNC6CQuAVq7MRJTZgTIbUaJhv/32i1NPPbXaa7Hppps22FcxcuTIePHFF6sZizl9+OGH8dJLL8WECROqWYVtttlm9m3t27ePHj16zLMcql6ZTWjXrl0VA41VxjBlypT46le/2uB4mTXZYostqr+XmY85x1HURwgAC5+wAGjlyt6D8tv9EhBlL0UJgXplxmJOkyZNiq222iquv/76eZ5nxRVX/NRLrxZUGUdx5513xmqrrdbgtrJHA4DmJywAWrkSD2WzdGNsueWWcdNNN1XLkpZZZpn53meVVVaJRx55JHbYYYfq+3Lq2uHDh1ePnZ8yK1JmSsreiPqlUHOqnzEpm8LrbbzxxlVAvPLKKx8507HRRhtVm9Dn9PDDDzfqfQKw4GzeBqDRvvOd70S3bt2qM0GVzdtjxoyp9lb88Ic/jNdee626z9FHHx1nnXVW3HbbbfHss8/GD37wg4+9BsVaa60Vffr0ib59+1aPqX/Osu+iKGerKvs5ypKtsu+jzFaUpVj9+/evNmxfffXV1TKsJ554In71q19V3xeHHXZYvPDCC/GjH/2o2vh9ww03VJvKAWgawgKARltyySXjvvvuizXWWKPanF1mBb73ve9VeyzqZzD69esX3/3ud6tYKHsaSgTsscceH/u8ZSnWXnvtVUXIhhtuGIccckhMnjy5uq0sdRowYEB1RqeVV145jjzyyOp4ucDeSSedVJ0dqoyjnJmqLI0qp58tyhjLGaVKrJRT0ZZN5GXDNwBNo03dR+2mAwAAaCQzFgAAQJqwAAAA0oQFAACQJiwAAIA0YQEAAKQJCwAAIE1YAAAAacICAABIExYAAECasAAAANKEBQAAkCYsAACAyPp/vAYADprVA5sAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, glcm_features, labels in test_loader:\n",
    "        images, glcm_features, labels = images.to(device), glcm_features.to(device), labels.to(device)\n",
    "        outputs = model(images, glcm_features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Hitung precision, recall, f1 untuk setiap kelas\n",
    "precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "\n",
    "# Tampilkan nilai precision, recall, dan f1-score per kelas\n",
    "num_classes = len(set(all_labels))  # Jumlah kelas yang ada di dataset\n",
    "for i in range(num_classes):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Precision: {precision[i]:.4f}\")\n",
    "    print(f\"  Recall:    {recall[i]:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1[i]:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Tampilkan classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(all_labels, all_preds, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Visualisasi Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), models_path+'/hybrid_glcm_cnn_modelv2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
