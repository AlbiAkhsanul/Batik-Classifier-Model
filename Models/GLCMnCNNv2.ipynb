{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv  \n",
    "\n",
    "load_dotenv()\n",
    "datasets_path = os.getenv('AUGMENTED_PATH_JOGJA_PEKALONGAN')\n",
    "models_path = os.getenv('MODELS_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jogja', 'pekalongan']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(datasets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "batch_size = 16\n",
    "test_split_ratio = 0.2\n",
    "val_split_ratio = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset With GLCM Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class GLCMCNNHybridDataset(Dataset):\n",
    "    def __init__(self, image_folder_dataset, transform=None):\n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.transform = transform\n",
    "        # Simpan fitur GLCM dan label\n",
    "        self.glcm_features = []\n",
    "        self.labels = []\n",
    "\n",
    "        print(\"ðŸ”„ Preprocessing GLCM features...\")\n",
    "\n",
    "        for idx in range(len(image_folder_dataset)):\n",
    "            image, label = image_folder_dataset[idx]\n",
    "\n",
    "            # Simpan label\n",
    "            self.labels.append(label)\n",
    "\n",
    "            # Transform dulu (pakai transform yg sama dengan training)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            # Convert tensor (C, H, W) -> numpy image (H, W, C)\n",
    "            image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "            image_np = (image_np * 255).astype(np.uint8)\n",
    "\n",
    "            # Ekstrak GLCM\n",
    "            gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "            distances = [1, 2, 3]\n",
    "            angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "            glcm = graycomatrix(gray, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "            props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
    "            features = []\n",
    "            for prop in props:\n",
    "                values = graycoprops(glcm, prop)\n",
    "                features.extend(values.flatten())\n",
    "            features = np.array(features, dtype=np.float32)\n",
    "    \n",
    "            self.glcm_features.append(features)\n",
    "\n",
    "        self.glcm_features = np.array(self.glcm_features, dtype=np.float32)\n",
    "        scaler = StandardScaler()\n",
    "        self.glcm_features = scaler.fit_transform(self.glcm_features)\n",
    "        self.glcm_features = torch.tensor(self.glcm_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "        print(\"âœ… GLCM features extracted and cached.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, _ = self.image_folder_dataset[index]  # label diambil dari self.labels\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        glcm_feature = self.glcm_features[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return image, glcm_feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Preprocessing GLCM features...\n",
      "âœ… GLCM features extracted and cached.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Buat dataset dasar\n",
    "base_dataset = datasets.ImageFolder(root=datasets_path)\n",
    "hybrid_dataset = GLCMCNNHybridDataset(base_dataset, transform=transform)\n",
    "\n",
    "# Ambil semua label\n",
    "targets = base_dataset.targets\n",
    "indices = list(range(len(base_dataset)))\n",
    "\n",
    "# Pertama: Bagi data menjadi train+val dan test\n",
    "trainval_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=test_split_ratio,\n",
    "    stratify=targets,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Ambil label untuk data trainval\n",
    "trainval_targets = [targets[i] for i in trainval_indices]\n",
    "\n",
    "# Kedua: Bagi trainval menjadi train dan val\n",
    "train_indices, val_indices = train_test_split(\n",
    "    trainval_indices,\n",
    "    test_size=val_split_ratio,\n",
    "    stratify=trainval_targets,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Buat subset dataset-nya\n",
    "train_dataset = Subset(hybrid_dataset, train_indices)\n",
    "val_dataset = Subset(hybrid_dataset, val_indices)\n",
    "test_dataset = Subset(hybrid_dataset, test_indices)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape GLCM Features: torch.Size([16, 60])\n"
     ]
    }
   ],
   "source": [
    "# Cek dimensi glcm_features\n",
    "for images, glcm_features, labels in train_loader:\n",
    "    print(\"Shape GLCM Features:\", glcm_features.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model (EfficientNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EfficientNet-B0\n",
    "weights = EfficientNet_B0_Weights.DEFAULT\n",
    "cnn_model = efficientnet_b0(weights=weights)\n",
    "cnn_feature_size = cnn_model.classifier[1].in_features  # EfficientNet features\n",
    "\n",
    "# Replace classification layer to get features\n",
    "cnn_model.classifier = nn.Identity()\n",
    "cnn_model = cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n"
     ]
    }
   ],
   "source": [
    "print(cnn_feature_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PararelModel(nn.Module):\n",
    "    def __init__(self, cnn_model, glcm_feature_size, cnn_feature_size, num_classes):\n",
    "        super(PararelModel, self).__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.fc1 = nn.Linear(glcm_feature_size + cnn_feature_size, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, image, glcm_features):\n",
    "        cnn_features = self.cnn_model(image)\n",
    "        combined_features = torch.cat((glcm_features, cnn_features), dim=1)\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleParallelModel(nn.Module):\n",
    "    def __init__(self, cnn_model, glcm_feature_size, cnn_feature_size, num_classes):\n",
    "        super(SimpleParallelModel, self).__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.fc1 = nn.Linear(glcm_feature_size + cnn_feature_size, 256)  # Menurunkan dimensi\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Langsung ke output class\n",
    "\n",
    "    def forward(self, image, glcm_features):\n",
    "        cnn_features = self.cnn_model(image)\n",
    "        combined_features = torch.cat((glcm_features, cnn_features), dim=1)\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "glcm_feature_size = 60  # Number of GLCM features\n",
    "# model = PararelModel(cnn_model, glcm_feature_size, cnn_feature_size, num_classes).to(device)\n",
    "model = SimpleParallelModel(cnn_model, glcm_feature_size, cnn_feature_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.6559, Val Acc: 0.7368\n",
      "Epoch [2/50], Loss: 0.5495, Val Acc: 0.8092\n",
      "Epoch [3/50], Loss: 0.4624, Val Acc: 0.8553\n",
      "Epoch [4/50], Loss: 0.3762, Val Acc: 0.8750\n",
      "Epoch [5/50], Loss: 0.3013, Val Acc: 0.8947\n",
      "Epoch [6/50], Loss: 0.2829, Val Acc: 0.9211\n",
      "Epoch [7/50], Loss: 0.2067, Val Acc: 0.9342\n",
      "Epoch [8/50], Loss: 0.1634, Val Acc: 0.9342\n",
      "Epoch [9/50], Loss: 0.1090, Val Acc: 0.9474\n",
      "Epoch [10/50], Loss: 0.1058, Val Acc: 0.9474\n",
      "Epoch [11/50], Loss: 0.0953, Val Acc: 0.9408\n",
      "Epoch [12/50], Loss: 0.1057, Val Acc: 0.9474\n",
      "Epoch [13/50], Loss: 0.1335, Val Acc: 0.9539\n",
      "Epoch [14/50], Loss: 0.0593, Val Acc: 0.9539\n",
      "Epoch [15/50], Loss: 0.0802, Val Acc: 0.9474\n",
      "Epoch [16/50], Loss: 0.0455, Val Acc: 0.9605\n",
      "Epoch [17/50], Loss: 0.0468, Val Acc: 0.9605\n",
      "Epoch [18/50], Loss: 0.0548, Val Acc: 0.9737\n",
      "Epoch [19/50], Loss: 0.0373, Val Acc: 0.9605\n",
      "Epoch [20/50], Loss: 0.0416, Val Acc: 0.9671\n",
      "Epoch [21/50], Loss: 0.0412, Val Acc: 0.9737\n",
      "Epoch [22/50], Loss: 0.0284, Val Acc: 0.9408\n",
      "Epoch [23/50], Loss: 0.0356, Val Acc: 0.9671\n",
      "Epoch [24/50], Loss: 0.0412, Val Acc: 0.9671\n",
      "Epoch [25/50], Loss: 0.0229, Val Acc: 0.9737\n",
      "Epoch [26/50], Loss: 0.0423, Val Acc: 0.9605\n",
      "Epoch [27/50], Loss: 0.0171, Val Acc: 0.9539\n",
      "Epoch [28/50], Loss: 0.0231, Val Acc: 0.9737\n",
      "Epoch [29/50], Loss: 0.0237, Val Acc: 0.9474\n",
      "Epoch [30/50], Loss: 0.0362, Val Acc: 0.9737\n",
      "Epoch [31/50], Loss: 0.0155, Val Acc: 0.9605\n",
      "Epoch [32/50], Loss: 0.0450, Val Acc: 0.9474\n",
      "Epoch [33/50], Loss: 0.0248, Val Acc: 0.9737\n",
      "Epoch [34/50], Loss: 0.0144, Val Acc: 0.9803\n",
      "Epoch [35/50], Loss: 0.0398, Val Acc: 0.9671\n",
      "Epoch [36/50], Loss: 0.0268, Val Acc: 0.9803\n",
      "Epoch [37/50], Loss: 0.0176, Val Acc: 0.9737\n",
      "Epoch [38/50], Loss: 0.0232, Val Acc: 0.9737\n",
      "Epoch [39/50], Loss: 0.0102, Val Acc: 0.9539\n",
      "Epoch [40/50], Loss: 0.0086, Val Acc: 0.9671\n",
      "Epoch [41/50], Loss: 0.0079, Val Acc: 0.9803\n",
      "Epoch [42/50], Loss: 0.0150, Val Acc: 0.9803\n",
      "Epoch [43/50], Loss: 0.0808, Val Acc: 0.9868\n",
      "Epoch [44/50], Loss: 0.0630, Val Acc: 0.9737\n",
      "Epoch [45/50], Loss: 0.0299, Val Acc: 0.9671\n",
      "Epoch [46/50], Loss: 0.0308, Val Acc: 0.9737\n",
      "â›” Early stopping triggered at epoch 46\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "patience = 5  # jumlah epoch tanpa perbaikan sebelum berhenti\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0015\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "#                                                        factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, glcm_features, labels in data_loader:\n",
    "            images, glcm_features, labels = images.to(device), glcm_features.to(device), labels.to(device)\n",
    "            outputs = model(images, glcm_features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Training loop dengan early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, glcm_features, labels in train_loader:\n",
    "        images, glcm_features, labels = images.to(device), glcm_features.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, glcm_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    val_acc = evaluate(model, test_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if epoch_loss < best_loss - 1e-4:\n",
    "        best_loss = epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'temp/best_model_hybrid.pth')  # Simpan model terbaik\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"â›” Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97        75\n",
      "           1       0.99      0.96      0.97        77\n",
      "\n",
      "    accuracy                           0.97       152\n",
      "   macro avg       0.97      0.97      0.97       152\n",
      "weighted avg       0.97      0.97      0.97       152\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, glcm_features, labels in test_loader:\n",
    "        images, glcm_features, labels = images.to(device), glcm_features.to(device), labels.to(device)\n",
    "        outputs = model(images, glcm_features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Hitung precision, recall, f1 untuk setiap kelas\n",
    "precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "\n",
    "# Tampilkan nilai precision, recall, dan f1-score per kelas\n",
    "num_classes = len(set(all_labels))  # Jumlah kelas yang ada di dataset\n",
    "# for i in range(num_classes):\n",
    "#     print(f\"Class {i}:\")\n",
    "#     print(f\"  Precision: {precision[i]:.4f}\")\n",
    "#     print(f\"  Recall:    {recall[i]:.4f}\")\n",
    "#     print(f\"  F1-Score:  {f1[i]:.4f}\")\n",
    "#     print(\"-\" * 30)\n",
    "\n",
    "# Tampilkan classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(all_labels, all_preds, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Visualisasi Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), models_path+'/hybrid_glcm_cnn_modelv2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
